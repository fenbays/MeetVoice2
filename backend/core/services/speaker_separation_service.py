import os
import ffmpeg
import time
import torch
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any, Callable
from pydub import AudioSegment
from funasr import AutoModel
from core.utils.model_manager import ModelManager
from core.utils.media_processor import MediaProcessor
from core.utils.download_manager import DownloadManager

class SpeakerSeparationService:
    """è¯´è¯äººåˆ†ç¦»æœåŠ¡"""
    
    def __init__(self, model_manager: ModelManager):
        self.model_manager = model_manager
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.hotwords = ""
        self._init_models()
        
    def _init_models(self):
        """åˆå§‹åŒ–è¯´è¯äººåˆ†ç¦»æ‰€éœ€çš„æ¨¡å‹"""
        try:
            print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è¯´è¯äººåˆ†ç¦»æ¨¡å‹...")
            
            # æ£€æŸ¥è¯´è¯äººåˆ†ç¦»æ¨¡å‹æ˜¯å¦å¯ç”¨
            download_manager = DownloadManager(self.model_manager.model_config)
            
            # æ£€æŸ¥ç¼ºå¤±çš„è¯´è¯äººåˆ†ç¦»æ¨¡å‹
            missing_models = download_manager.get_missing_speaker_models()
            if missing_models:
                print(f"âš ï¸ ç¼ºå¤±è¯´è¯äººåˆ†ç¦»æ¨¡å‹: {missing_models}")
                print("ğŸ”„ æ­£åœ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹...")
                
                results = download_manager.download_speaker_separation_models()
                failed_models = [name for name, success, _ in results if not success]
                
                if failed_models:
                    raise Exception(f"è¯´è¯äººåˆ†ç¦»æ¨¡å‹ä¸‹è½½å¤±è´¥: {failed_models}")
                
                print("âœ… è¯´è¯äººåˆ†ç¦»æ¨¡å‹ä¸‹è½½å®Œæˆ")
            
            # è·å–æ¨¡å‹è·¯å¾„
            config = self.model_manager.model_config
            asr_model_path = config.get_model_path("paraformer-zh-large")
            vad_model_path = config.get_model_path("fsmn-vad")
            punc_model_path = config.get_model_path("punc-transformer")
            spk_model_path = config.get_model_path("campplus-speaker")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            required_paths = {
                "ASRæ¨¡å‹": asr_model_path,
                "VADæ¨¡å‹": vad_model_path,
                "æ ‡ç‚¹æ¨¡å‹": punc_model_path,
                "è¯´è¯äººæ¨¡å‹": spk_model_path
            }
            
            for name, path in required_paths.items():
                if not path or not os.path.exists(path):
                    raise Exception(f"{name}è·¯å¾„æ— æ•ˆ: {path}")
            
            # è·å–æ¨¡å‹ç‰ˆæœ¬
            asr_version = config.get_model_version("paraformer-zh-large")
            vad_version = config.get_model_version("fsmn-vad")
            punc_version = config.get_model_version("punc-transformer")
            spk_version = config.get_model_version("campplus-speaker")
            
            # åˆ›å»ºAutoModelå®ä¾‹
            self.model = AutoModel(
                model=asr_model_path,
                model_revision=asr_version,
                vad_model=vad_model_path,
                vad_model_revision=vad_version,
                punc_model=punc_model_path,
                punc_model_revision=punc_version,
                spk_model=spk_model_path,
                spk_model_revision=spk_version,
                ngpu=1 if torch.cuda.is_available() else 0,
                ncpu=os.cpu_count(),
                device=self.device,
                disable_pbar=True,
                disable_log=True,
                disable_update=True
            )
            
            print(f"âœ… è¯´è¯äººåˆ†ç¦»æ¨¡å‹åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
            print(f"   ğŸ“¦ ASRæ¨¡å‹: {os.path.basename(asr_model_path)} ({asr_version})")
            print(f"   ğŸ“¦ VADæ¨¡å‹: {os.path.basename(vad_model_path)} ({vad_version})")
            print(f"   ğŸ“¦ æ ‡ç‚¹æ¨¡å‹: {os.path.basename(punc_model_path)} ({punc_version})")
            print(f"   ğŸ“¦ è¯´è¯äººæ¨¡å‹: {os.path.basename(spk_model_path)} ({spk_version})")
            
        except Exception as e:
            print(f"âŒ è¯´è¯äººåˆ†ç¦»æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e
    
    def set_hotwords(self, hotwords_file: Optional[str] = None, hotwords_list: Optional[List[str]] = None):
        """è®¾ç½®çƒ­è¯"""
        if hotwords_file and os.path.exists(hotwords_file):
            with open(hotwords_file, "r", encoding="utf-8") as f:
                lines = f.readlines()
                lines = [line.strip() for line in lines if line.strip()]
            self.hotwords = " ".join(lines)
            print(f"ğŸ“ ä»æ–‡ä»¶åŠ è½½çƒ­è¯: {self.hotwords}")
        elif hotwords_list:
            self.hotwords = " ".join(hotwords_list)
            print(f"ğŸ“ è®¾ç½®çƒ­è¯: {self.hotwords}")
        else:
            self.hotwords = ""
    
    def _to_date(self, milliseconds: int) -> str:
        """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºSRTæ ¼å¼çš„æ—¶é—´"""
        time_obj = timedelta(milliseconds=milliseconds)
        hours = time_obj.seconds // 3600
        minutes = (time_obj.seconds // 60) % 60
        seconds = time_obj.seconds % 60
        microseconds = time_obj.microseconds // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{microseconds:03d}"
    
    def _preprocess_audio(self, audio_file: str) -> bytes:
        """é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€æ ¼å¼"""
        try:
            print(f"ğŸ”„ é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_file}")
            
            # ä½¿ç”¨ffmpegé¢„å¤„ç†éŸ³é¢‘
            audio_bytes, _ = (
                ffmpeg.input(audio_file, threads=0)
                .output("-", format="wav", acodec="pcm_s16le", ac=1, ar=16000)
                .run(cmd=["ffmpeg", "-nostdin"], capture_stdout=True, capture_stderr=True)
            )
            
            print(f"âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆ")
            return audio_bytes
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
            raise e
    
    def separate_speakers(self, 
                         audio_file: str,
                         merge_threshold: int = 10,
                         progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            merge_threshold: åˆå¹¶ç›¸é‚»ç›¸åŒè¯´è¯äººçš„å­—æ•°é˜ˆå€¼
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            åŒ…å«åˆ†ç¦»ç»“æœçš„å­—å…¸
        """
        if not os.path.exists(audio_file):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        
        try:
            if progress_callback:
                progress_callback("å¼€å§‹è¯´è¯äººåˆ†ç¦»...", 0)
            
            # é¢„å¤„ç†éŸ³é¢‘
            audio_bytes = self._preprocess_audio(audio_file)
            
            if progress_callback:
                progress_callback("æ­£åœ¨æ‰§è¡Œè¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ç¦»...", 30)
            
            # æ‰§è¡Œè¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ç¦»
            start_time = time.time()
            res = self.model.generate(
                input=audio_bytes, 
                batch_size_s=300, 
                is_final=True, 
                sentence_timestamp=True, 
                hotword=self.hotwords
            )
            end_time = time.time()
            
            if progress_callback:
                progress_callback("å¤„ç†è¯†åˆ«ç»“æœ...", 70)
            
            rec_result = res[0]
            asr_result_text = rec_result['text']
            
            if not asr_result_text:
                print("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³å†…å®¹")
                return {
                    'success': False,
                    'message': 'æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³å†…å®¹',
                    'full_text': '',
                    'speakers': {},
                    'sentences': [],
                    'processing_time': end_time - start_time
                }
            
            # å¤„ç†å¥å­ä¿¡æ¯ï¼Œåˆå¹¶ç›¸é‚»ç›¸åŒè¯´è¯äººçš„çŸ­å¥
            sentences = []
            for sentence in rec_result["sentence_info"]:
                start = self._to_date(sentence["start"])
                end = self._to_date(sentence["end"])
                
                # å¦‚æœæ˜¯ç›¸åŒè¯´è¯äººä¸”å½“å‰å¥å­é•¿åº¦å°äºé˜ˆå€¼ï¼Œåˆ™åˆå¹¶
                if (sentences and 
                    sentence["spk"] == sentences[-1]["spk"] and 
                    len(sentences[-1]["text"]) < merge_threshold):
                    sentences[-1]["text"] += sentence["text"]
                    sentences[-1]["end"] = end
                else:
                    sentences.append({
                        "text": sentence["text"],
                        "start": start,
                        "end": end,
                        "spk": sentence["spk"]
                    })
            
            # æŒ‰è¯´è¯äººåˆ†ç»„
            speakers = {}
            for sentence in sentences:
                spk_id = sentence["spk"]
                if spk_id not in speakers:
                    speakers[spk_id] = {
                        'segments': [],
                        'total_text': '',
                        'total_duration': 0
                    }
                
                speakers[spk_id]['segments'].append({
                    'text': sentence['text'],
                    'start': sentence['start'],
                    'end': sentence['end']
                })
                speakers[spk_id]['total_text'] += sentence['text']
            
            if progress_callback:
                progress_callback("åˆ†ç¦»å®Œæˆ", 100)
            
            print(f"âœ… è¯´è¯äººåˆ†ç¦»å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š æ£€æµ‹åˆ° {len(speakers)} ä¸ªè¯´è¯äºº")
            for spk_id, info in speakers.items():
                print(f"   è¯´è¯äºº {spk_id}: {len(info['segments'])} ä¸ªç‰‡æ®µ")
            
            return {
                'success': True,
                'message': f'æˆåŠŸåˆ†ç¦»å‡º {len(speakers)} ä¸ªè¯´è¯äºº',
                'full_text': asr_result_text,
                'speakers': speakers,
                'sentences': sentences,
                'processing_time': end_time - start_time,
                'audio_file': audio_file
            }
            
        except Exception as e:
            error_msg = f"è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            if progress_callback:
                progress_callback(f"é”™è¯¯: {error_msg}", -1)
            return {
                'success': False,
                'message': error_msg,
                'full_text': '',
                'speakers': {},
                'sentences': [],
                'processing_time': 0
            }
    
    def save_separation_results(self, 
                              separation_result: Dict[str, Any], 
                              output_dir: str,
                              save_audio_segments: bool = True,
                              save_merged_audio: bool = True) -> Dict[str, str]:
        """
        ä¿å­˜è¯´è¯äººåˆ†ç¦»ç»“æœ
        
        Args:
            separation_result: åˆ†ç¦»ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            save_audio_segments: æ˜¯å¦ä¿å­˜éŸ³é¢‘ç‰‡æ®µ
            save_merged_audio: æ˜¯å¦ä¿å­˜åˆå¹¶çš„éŸ³é¢‘
            
        Returns:
            ä¿å­˜è·¯å¾„ä¿¡æ¯
        """
        if not separation_result['success']:
            raise ValueError("åˆ†ç¦»ç»“æœæ— æ•ˆ")
        
        try:
            audio_file = separation_result['audio_file']
            audio_name = os.path.splitext(os.path.basename(audio_file))[0]
            speakers = separation_result['speakers']
            sentences = separation_result['sentences']
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            date = datetime.now().strftime("%Y-%m-%d")
            final_output_dir = os.path.join(output_dir, date, audio_name)
            os.makedirs(final_output_dir, exist_ok=True)
            
            saved_paths = {
                'base_dir': final_output_dir,
                'text_files': {},
                'audio_segments': {},
                'merged_audio': {}
            }
            
            # ä¿å­˜æ¯ä¸ªè¯´è¯äººçš„æ–‡æœ¬
            for spk_id, info in speakers.items():
                # åˆ›å»ºè¯´è¯äººç›®å½•
                spk_dir = os.path.join(final_output_dir, f"speaker_{spk_id}")
                os.makedirs(spk_dir, exist_ok=True)
                
                # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
                text_file = os.path.join(spk_dir, f"speaker_{spk_id}.txt")
                with open(text_file, 'w', encoding='utf-8') as f:
                    for segment in info['segments']:
                        f.write(f"{segment['start']} --> {segment['end']}\n")
                        f.write(f"{segment['text']}\n\n")
                
                saved_paths['text_files'][spk_id] = text_file
                
                # ä¿å­˜éŸ³é¢‘ç‰‡æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰
                if save_audio_segments:
                    segment_files = []
                    for i, segment in enumerate(info['segments']):
                        segment_file = os.path.join(spk_dir, f"segment_{i:03d}.wav")
                        self._extract_audio_segment(
                            audio_file, 
                            segment['start'], 
                            segment['end'], 
                            segment_file
                        )
                        segment_files.append(segment_file)
                    
                    saved_paths['audio_segments'][spk_id] = segment_files
                    
                    # åˆå¹¶è¯´è¯äººçš„æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if save_merged_audio and segment_files:
                        merged_file = os.path.join(spk_dir, f"speaker_{spk_id}_merged.mp3")
                        self._merge_audio_segments(segment_files, merged_file)
                        saved_paths['merged_audio'][spk_id] = merged_file
            
            # ä¿å­˜å®Œæ•´çš„åˆ†ç¦»æŠ¥å‘Š
            report_file = os.path.join(final_output_dir, "separation_report.txt")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(f"è¯´è¯äººåˆ†ç¦»æŠ¥å‘Š\n")
                f.write(f"=" * 50 + "\n")
                f.write(f"éŸ³é¢‘æ–‡ä»¶: {audio_file}\n")
                f.write(f"å¤„ç†æ—¶é—´: {separation_result['processing_time']:.2f}ç§’\n")
                f.write(f"æ£€æµ‹åˆ°è¯´è¯äººæ•°é‡: {len(speakers)}\n\n")
                
                for spk_id, info in speakers.items():
                    f.write(f"è¯´è¯äºº {spk_id}:\n")
                    f.write(f"  ç‰‡æ®µæ•°é‡: {len(info['segments'])}\n")
                    f.write(f"  æ€»æ–‡æœ¬: {info['total_text'][:100]}...\n\n")
            
            saved_paths['report'] = report_file
            
            print(f"âœ… åˆ†ç¦»ç»“æœå·²ä¿å­˜åˆ°: {final_output_dir}")
            return saved_paths
            
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†ç¦»ç»“æœå¤±è´¥: {e}")
            raise e
    
    def _extract_audio_segment(self, audio_file: str, start_time: str, end_time: str, output_file: str):
        """æå–éŸ³é¢‘ç‰‡æ®µ"""
        try:
            (
                ffmpeg.input(audio_file, ss=start_time, to=end_time)
                .output(output_file)
                .run(cmd=["ffmpeg", "-nostdin"], overwrite_output=True, capture_stdout=True, capture_stderr=True)
            )
        except Exception as e:
            print(f"âš ï¸ æå–éŸ³é¢‘ç‰‡æ®µå¤±è´¥: {e}")
    
    def _merge_audio_segments(self, segment_files: List[str], output_file: str):
        """åˆå¹¶éŸ³é¢‘ç‰‡æ®µ"""
        try:
            if not segment_files:
                return
            
            # ä½¿ç”¨pydubåˆå¹¶éŸ³é¢‘
            combined = AudioSegment.from_file(segment_files[0])
            for segment_file in segment_files[1:]:
                if os.path.exists(segment_file):
                    segment = AudioSegment.from_file(segment_file)
                    combined += segment
            
            combined.export(output_file, format="mp3")
            print(f"âœ… éŸ³é¢‘ç‰‡æ®µå·²åˆå¹¶åˆ°: {output_file}")
            
        except Exception as e:
            print(f"âš ï¸ åˆå¹¶éŸ³é¢‘ç‰‡æ®µå¤±è´¥: {e}")
    
    def batch_separate_speakers(self, 
                              audio_files: List[str], 
                              output_dir: str,
                              merge_threshold: int = 10,
                              progress_callback: Optional[Callable] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶çš„è¯´è¯äººåˆ†ç¦»
        
        Args:
            audio_files: éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            merge_threshold: åˆå¹¶é˜ˆå€¼
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        total_files = len(audio_files)
        
        for i, audio_file in enumerate(audio_files):
            try:
                if progress_callback:
                    progress_callback(f"å¤„ç†æ–‡ä»¶ {i+1}/{total_files}: {os.path.basename(audio_file)}", 
                                    int((i / total_files) * 100))
                
                print(f"\nğŸ”„ å¤„ç†æ–‡ä»¶ {i+1}/{total_files}: {audio_file}")
                
                # æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
                result = self.separate_speakers(audio_file, merge_threshold)
                
                if result['success']:
                    # ä¿å­˜ç»“æœ
                    saved_paths = self.save_separation_results(result, output_dir)
                    result['saved_paths'] = saved_paths
                
                results.append(result)
                
            except Exception as e:
                error_result = {
                    'success': False,
                    'message': f'å¤„ç†æ–‡ä»¶å¤±è´¥: {e}',
                    'audio_file': audio_file
                }
                results.append(error_result)
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {audio_file}, é”™è¯¯: {e}")
        
        if progress_callback:
            progress_callback("æ‰¹é‡å¤„ç†å®Œæˆ", 100)
        
        return results
